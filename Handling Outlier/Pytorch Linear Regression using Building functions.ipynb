{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Inputs=np.array(\n",
    "                [[73,67,43], [91,88,64],[87,134,58],\n",
    "                 [102,43,37], [69,96,70], [73,67,43],\n",
    "                 [91,88,64],  [87,134,58],[102,43,37],\n",
    "                 [69,96,70], [73,67,43], [91,88,64],\n",
    "                 [87,134,58], [102,43,37], [69,96,70]\n",
    "                ], dtype='float32'\n",
    ")\n",
    "\n",
    "Targets=np.array([\n",
    "                [56,70], [81,101], [119,133],\n",
    "                [22,37], [103,119], [56,70],\n",
    "                [81,101], [119,133], [22,37],\n",
    "                [103,119], [56,70], [81,101],\n",
    "                [119,133], [22,37], [103,119]\n",
    "],dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=torch.from_numpy(Inputs)\n",
    "targets=torch.from_numpy(Targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataset\n",
    "train_ds=TensorDataset(inputs,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.]]),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=5\n",
    "train_dl=DataLoader(train_ds,batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([73., 67., 43.])\n",
      "tensor([56., 70.])\n",
      "tensor([91., 88., 64.])\n",
      "tensor([ 81., 101.])\n",
      "tensor([ 87., 134.,  58.])\n",
      "tensor([119., 133.])\n",
      "tensor([102.,  43.,  37.])\n",
      "tensor([22., 37.])\n",
      "tensor([69., 96., 70.])\n",
      "tensor([103., 119.])\n"
     ]
    }
   ],
   "source": [
    "for xb,yb in train_ds:\n",
    "    print(xb)\n",
    "    print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.5142,  0.0513,  0.3743],\n",
      "        [-0.1963,  0.3037,  0.3643]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2173, -0.3164], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#Define model\n",
    "\n",
    "model=nn.Linear(3,2)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.5142,  0.0513,  0.3743],\n",
       "         [-0.1963,  0.3037,  0.3643]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.2173, -0.3164], requires_grad=True)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[57.2840, 21.3668],\n",
       "        [75.4765, 31.8619],\n",
       "        [73.5322, 44.4330],\n",
       "        [68.7201,  6.1973],\n",
       "        [66.8196, 40.7973]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can use the model to generate predictions in the same way as before.\n",
    "preds=model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import nn.functional\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Define loss function\n",
    "loss_fn=F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2764.5103, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss=loss_fn(preds,targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(num_epoch,model,loss_fn,opt):\n",
    "    \n",
    "    #Repeat for number of epochs given\n",
    "    for epoch in range(num_epoch):\n",
    "        \n",
    "        #Train with batches of the data\n",
    "        for xb,yb in train_dl:\n",
    "            \n",
    "            #generate predictions\n",
    "            preds=model(xb)\n",
    "            \n",
    "            #generate loss function\n",
    "            loss=loss_fn(preds,yb)\n",
    "            \n",
    "            #compute gradient\n",
    "            loss.backward()\n",
    "            \n",
    "            #update parameters using gtadients\n",
    "            opt.step()\n",
    "            \n",
    "            #reset gradient to zero\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        print(\"Epoch: {}   Loss: {}\". format(epoch, loss.item()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0   Loss: 374.47357177734375\n",
      "Epoch: 1   Loss: 369.8227844238281\n",
      "Epoch: 2   Loss: 365.23162841796875\n",
      "Epoch: 3   Loss: 360.69903564453125\n",
      "Epoch: 4   Loss: 356.2242431640625\n",
      "Epoch: 5   Loss: 351.80596923828125\n",
      "Epoch: 6   Loss: 347.44366455078125\n",
      "Epoch: 7   Loss: 343.13641357421875\n",
      "Epoch: 8   Loss: 338.8834533691406\n",
      "Epoch: 9   Loss: 334.6842346191406\n",
      "Epoch: 10   Loss: 330.5379943847656\n",
      "Epoch: 11   Loss: 326.44384765625\n",
      "Epoch: 12   Loss: 322.40142822265625\n",
      "Epoch: 13   Loss: 318.4099426269531\n",
      "Epoch: 14   Loss: 314.468505859375\n",
      "Epoch: 15   Loss: 310.576904296875\n",
      "Epoch: 16   Loss: 306.73431396484375\n",
      "Epoch: 17   Loss: 302.9400634765625\n",
      "Epoch: 18   Loss: 299.193603515625\n",
      "Epoch: 19   Loss: 295.4942932128906\n",
      "Epoch: 20   Loss: 291.8415832519531\n",
      "Epoch: 21   Loss: 288.2348327636719\n",
      "Epoch: 22   Loss: 284.6734924316406\n",
      "Epoch: 23   Loss: 281.1569519042969\n",
      "Epoch: 24   Loss: 277.6848449707031\n",
      "Epoch: 25   Loss: 274.2563781738281\n",
      "Epoch: 26   Loss: 270.8709716796875\n",
      "Epoch: 27   Loss: 267.5282897949219\n",
      "Epoch: 28   Loss: 264.2275390625\n",
      "Epoch: 29   Loss: 260.96844482421875\n",
      "Epoch: 30   Loss: 257.75042724609375\n",
      "Epoch: 31   Loss: 254.5728759765625\n",
      "Epoch: 32   Loss: 251.43533325195312\n",
      "Epoch: 33   Loss: 248.3372039794922\n",
      "Epoch: 34   Loss: 245.278076171875\n",
      "Epoch: 35   Loss: 242.257568359375\n",
      "Epoch: 36   Loss: 239.2749481201172\n",
      "Epoch: 37   Loss: 236.3299102783203\n",
      "Epoch: 38   Loss: 233.42190551757812\n",
      "Epoch: 39   Loss: 230.55050659179688\n",
      "Epoch: 40   Loss: 227.71524047851562\n",
      "Epoch: 41   Loss: 224.91555786132812\n",
      "Epoch: 42   Loss: 222.1511993408203\n",
      "Epoch: 43   Loss: 219.4215850830078\n",
      "Epoch: 44   Loss: 216.726318359375\n",
      "Epoch: 45   Loss: 214.06497192382812\n",
      "Epoch: 46   Loss: 211.4371337890625\n",
      "Epoch: 47   Loss: 208.8422088623047\n",
      "Epoch: 48   Loss: 206.28012084960938\n",
      "Epoch: 49   Loss: 203.75015258789062\n",
      "Epoch: 50   Loss: 201.25198364257812\n",
      "Epoch: 51   Loss: 198.78518676757812\n",
      "Epoch: 52   Loss: 196.34962463378906\n",
      "Epoch: 53   Loss: 193.94448852539062\n",
      "Epoch: 54   Loss: 191.56967163085938\n",
      "Epoch: 55   Loss: 189.22470092773438\n",
      "Epoch: 56   Loss: 186.90921020507812\n",
      "Epoch: 57   Loss: 184.622802734375\n",
      "Epoch: 58   Loss: 182.365234375\n",
      "Epoch: 59   Loss: 180.13597106933594\n",
      "Epoch: 60   Loss: 177.9347686767578\n",
      "Epoch: 61   Loss: 175.76129150390625\n",
      "Epoch: 62   Loss: 173.61495971679688\n",
      "Epoch: 63   Loss: 171.4957275390625\n",
      "Epoch: 64   Loss: 169.40318298339844\n",
      "Epoch: 65   Loss: 167.33688354492188\n",
      "Epoch: 66   Loss: 165.29652404785156\n",
      "Epoch: 67   Loss: 163.2818145751953\n",
      "Epoch: 68   Loss: 161.29237365722656\n",
      "Epoch: 69   Loss: 159.32801818847656\n",
      "Epoch: 70   Loss: 157.3883514404297\n",
      "Epoch: 71   Loss: 155.47305297851562\n",
      "Epoch: 72   Loss: 153.5817108154297\n",
      "Epoch: 73   Loss: 151.7142333984375\n",
      "Epoch: 74   Loss: 149.87014770507812\n",
      "Epoch: 75   Loss: 148.04928588867188\n",
      "Epoch: 76   Loss: 146.2512969970703\n",
      "Epoch: 77   Loss: 144.47586059570312\n",
      "Epoch: 78   Loss: 142.7227020263672\n",
      "Epoch: 79   Loss: 140.99160766601562\n",
      "Epoch: 80   Loss: 139.28221130371094\n",
      "Epoch: 81   Loss: 137.59426879882812\n",
      "Epoch: 82   Loss: 135.92755126953125\n",
      "Epoch: 83   Loss: 134.2818145751953\n",
      "Epoch: 84   Loss: 132.65664672851562\n",
      "Epoch: 85   Loss: 131.0519561767578\n",
      "Epoch: 86   Loss: 129.4673614501953\n",
      "Epoch: 87   Loss: 127.9026870727539\n",
      "Epoch: 88   Loss: 126.3576889038086\n",
      "Epoch: 89   Loss: 124.83195495605469\n",
      "Epoch: 90   Loss: 123.3254623413086\n",
      "Epoch: 91   Loss: 121.83782958984375\n",
      "Epoch: 92   Loss: 120.36894226074219\n",
      "Epoch: 93   Loss: 118.91838073730469\n",
      "Epoch: 94   Loss: 117.4861068725586\n",
      "Epoch: 95   Loss: 116.07173156738281\n",
      "Epoch: 96   Loss: 114.67512512207031\n",
      "Epoch: 97   Loss: 113.29605865478516\n",
      "Epoch: 98   Loss: 111.93428039550781\n",
      "Epoch: 99   Loss: 110.58953857421875\n",
      "Epoch: 100   Loss: 109.26173400878906\n",
      "Epoch: 101   Loss: 107.95052337646484\n",
      "Epoch: 102   Loss: 106.65583801269531\n",
      "Epoch: 103   Loss: 105.37725830078125\n",
      "Epoch: 104   Loss: 104.11479187011719\n",
      "Epoch: 105   Loss: 102.8681411743164\n",
      "Epoch: 106   Loss: 101.63710021972656\n",
      "Epoch: 107   Loss: 100.42149353027344\n",
      "Epoch: 108   Loss: 99.22111511230469\n",
      "Epoch: 109   Loss: 98.03582000732422\n",
      "Epoch: 110   Loss: 96.86534881591797\n",
      "Epoch: 111   Loss: 95.70954895019531\n",
      "Epoch: 112   Loss: 94.5682144165039\n",
      "Epoch: 113   Loss: 93.44117736816406\n",
      "Epoch: 114   Loss: 92.32830810546875\n",
      "Epoch: 115   Loss: 91.22937774658203\n",
      "Epoch: 116   Loss: 90.1441879272461\n",
      "Epoch: 117   Loss: 89.07252502441406\n",
      "Epoch: 118   Loss: 88.01435852050781\n",
      "Epoch: 119   Loss: 86.96939849853516\n",
      "Epoch: 120   Loss: 85.93756103515625\n",
      "Epoch: 121   Loss: 84.91862487792969\n",
      "Epoch: 122   Loss: 83.9124755859375\n",
      "Epoch: 123   Loss: 82.9188461303711\n",
      "Epoch: 124   Loss: 81.93766784667969\n",
      "Epoch: 125   Loss: 80.96883392333984\n",
      "Epoch: 126   Loss: 80.01209259033203\n",
      "Epoch: 127   Loss: 79.06727600097656\n",
      "Epoch: 128   Loss: 78.13433837890625\n",
      "Epoch: 129   Loss: 77.2130355834961\n",
      "Epoch: 130   Loss: 76.30326843261719\n",
      "Epoch: 131   Loss: 75.40489196777344\n",
      "Epoch: 132   Loss: 74.51773834228516\n",
      "Epoch: 133   Loss: 73.64169311523438\n",
      "Epoch: 134   Loss: 72.77659606933594\n",
      "Epoch: 135   Loss: 71.92228698730469\n",
      "Epoch: 136   Loss: 71.07868957519531\n",
      "Epoch: 137   Loss: 70.24559020996094\n",
      "Epoch: 138   Loss: 69.42293548583984\n",
      "Epoch: 139   Loss: 68.61055755615234\n",
      "Epoch: 140   Loss: 67.8083724975586\n",
      "Epoch: 141   Loss: 67.01615142822266\n",
      "Epoch: 142   Loss: 66.23381042480469\n",
      "Epoch: 143   Loss: 65.46128845214844\n",
      "Epoch: 144   Loss: 64.6983871459961\n",
      "Epoch: 145   Loss: 63.9450798034668\n",
      "Epoch: 146   Loss: 63.20109939575195\n",
      "Epoch: 147   Loss: 62.4664192199707\n",
      "Epoch: 148   Loss: 61.74092483520508\n",
      "Epoch: 149   Loss: 61.0245246887207\n",
      "Epoch: 150   Loss: 60.31699752807617\n",
      "Epoch: 151   Loss: 59.61832809448242\n",
      "Epoch: 152   Loss: 58.92839813232422\n",
      "Epoch: 153   Loss: 58.24700927734375\n",
      "Epoch: 154   Loss: 57.574188232421875\n",
      "Epoch: 155   Loss: 56.90973663330078\n",
      "Epoch: 156   Loss: 56.253570556640625\n",
      "Epoch: 157   Loss: 55.6055908203125\n",
      "Epoch: 158   Loss: 54.96564865112305\n",
      "Epoch: 159   Loss: 54.33372116088867\n",
      "Epoch: 160   Loss: 53.70969772338867\n",
      "Epoch: 161   Loss: 53.093406677246094\n",
      "Epoch: 162   Loss: 52.48479461669922\n",
      "Epoch: 163   Loss: 51.88373947143555\n",
      "Epoch: 164   Loss: 51.29022979736328\n",
      "Epoch: 165   Loss: 50.704078674316406\n",
      "Epoch: 166   Loss: 50.12526321411133\n",
      "Epoch: 167   Loss: 49.55363464355469\n",
      "Epoch: 168   Loss: 48.98908233642578\n",
      "Epoch: 169   Loss: 48.43159866333008\n",
      "Epoch: 170   Loss: 47.88102722167969\n",
      "Epoch: 171   Loss: 47.3372917175293\n",
      "Epoch: 172   Loss: 46.80035400390625\n",
      "Epoch: 173   Loss: 46.27009201049805\n",
      "Epoch: 174   Loss: 45.7464599609375\n",
      "Epoch: 175   Loss: 45.22926712036133\n",
      "Epoch: 176   Loss: 44.718570709228516\n",
      "Epoch: 177   Loss: 44.21417236328125\n",
      "Epoch: 178   Loss: 43.71607971191406\n",
      "Epoch: 179   Loss: 43.22417449951172\n",
      "Epoch: 180   Loss: 42.738346099853516\n",
      "Epoch: 181   Loss: 42.258567810058594\n",
      "Epoch: 182   Loss: 41.784751892089844\n",
      "Epoch: 183   Loss: 41.316837310791016\n",
      "Epoch: 184   Loss: 40.854713439941406\n",
      "Epoch: 185   Loss: 40.398319244384766\n",
      "Epoch: 186   Loss: 39.94760513305664\n",
      "Epoch: 187   Loss: 39.50246047973633\n",
      "Epoch: 188   Loss: 39.06287384033203\n",
      "Epoch: 189   Loss: 38.628719329833984\n",
      "Epoch: 190   Loss: 38.199913024902344\n",
      "Epoch: 191   Loss: 37.776493072509766\n",
      "Epoch: 192   Loss: 37.358272552490234\n",
      "Epoch: 193   Loss: 36.94524383544922\n",
      "Epoch: 194   Loss: 36.5373420715332\n",
      "Epoch: 195   Loss: 36.13447570800781\n",
      "Epoch: 196   Loss: 35.73661804199219\n",
      "Epoch: 197   Loss: 35.34370040893555\n",
      "Epoch: 198   Loss: 34.95563507080078\n",
      "Epoch: 199   Loss: 34.572364807128906\n",
      "Epoch: 200   Loss: 34.1938362121582\n",
      "Epoch: 201   Loss: 33.820011138916016\n",
      "Epoch: 202   Loss: 33.45079803466797\n",
      "Epoch: 203   Loss: 33.08613204956055\n",
      "Epoch: 204   Loss: 32.725982666015625\n",
      "Epoch: 205   Loss: 32.37028503417969\n",
      "Epoch: 206   Loss: 32.01904296875\n",
      "Epoch: 207   Loss: 31.672069549560547\n",
      "Epoch: 208   Loss: 31.329397201538086\n",
      "Epoch: 209   Loss: 30.990962982177734\n",
      "Epoch: 210   Loss: 30.656742095947266\n",
      "Epoch: 211   Loss: 30.326562881469727\n",
      "Epoch: 212   Loss: 30.000560760498047\n",
      "Epoch: 213   Loss: 29.678546905517578\n",
      "Epoch: 214   Loss: 29.3604736328125\n",
      "Epoch: 215   Loss: 29.04632568359375\n",
      "Epoch: 216   Loss: 28.736093521118164\n",
      "Epoch: 217   Loss: 28.429622650146484\n",
      "Epoch: 218   Loss: 28.126962661743164\n",
      "Epoch: 219   Loss: 27.828052520751953\n",
      "Epoch: 220   Loss: 27.532772064208984\n",
      "Epoch: 221   Loss: 27.241174697875977\n",
      "Epoch: 222   Loss: 26.95314598083496\n",
      "Epoch: 223   Loss: 26.66867446899414\n",
      "Epoch: 224   Loss: 26.387704849243164\n",
      "Epoch: 225   Loss: 26.110164642333984\n",
      "Epoch: 226   Loss: 25.836078643798828\n",
      "Epoch: 227   Loss: 25.56534767150879\n",
      "Epoch: 228   Loss: 25.297922134399414\n",
      "Epoch: 229   Loss: 25.033790588378906\n",
      "Epoch: 230   Loss: 24.772924423217773\n",
      "Epoch: 231   Loss: 24.515228271484375\n",
      "Epoch: 232   Loss: 24.260732650756836\n",
      "Epoch: 233   Loss: 24.009326934814453\n",
      "Epoch: 234   Loss: 23.761009216308594\n",
      "Epoch: 235   Loss: 23.5157527923584\n",
      "Epoch: 236   Loss: 23.27347755432129\n",
      "Epoch: 237   Loss: 23.034183502197266\n",
      "Epoch: 238   Loss: 22.79782485961914\n",
      "Epoch: 239   Loss: 22.564367294311523\n",
      "Epoch: 240   Loss: 22.33376693725586\n",
      "Epoch: 241   Loss: 22.105985641479492\n",
      "Epoch: 242   Loss: 21.880990982055664\n",
      "Epoch: 243   Loss: 21.658748626708984\n",
      "Epoch: 244   Loss: 21.43923568725586\n",
      "Epoch: 245   Loss: 21.222408294677734\n",
      "Epoch: 246   Loss: 21.00820541381836\n",
      "Epoch: 247   Loss: 20.796649932861328\n",
      "Epoch: 248   Loss: 20.587657928466797\n",
      "Epoch: 249   Loss: 20.381187438964844\n",
      "Epoch: 250   Loss: 20.17727279663086\n",
      "Epoch: 251   Loss: 19.97584342956543\n",
      "Epoch: 252   Loss: 19.776870727539062\n",
      "Epoch: 253   Loss: 19.580307006835938\n",
      "Epoch: 254   Loss: 19.386157989501953\n",
      "Epoch: 255   Loss: 19.194368362426758\n",
      "Epoch: 256   Loss: 19.004894256591797\n",
      "Epoch: 257   Loss: 18.817773818969727\n",
      "Epoch: 258   Loss: 18.632883071899414\n",
      "Epoch: 259   Loss: 18.45024299621582\n",
      "Epoch: 260   Loss: 18.269832611083984\n",
      "Epoch: 261   Loss: 18.09161949157715\n",
      "Epoch: 262   Loss: 17.91558074951172\n",
      "Epoch: 263   Loss: 17.741655349731445\n",
      "Epoch: 264   Loss: 17.569862365722656\n",
      "Epoch: 265   Loss: 17.400148391723633\n",
      "Epoch: 266   Loss: 17.232500076293945\n",
      "Epoch: 267   Loss: 17.066850662231445\n",
      "Epoch: 268   Loss: 16.903247833251953\n",
      "Epoch: 269   Loss: 16.74160385131836\n",
      "Epoch: 270   Loss: 16.581907272338867\n",
      "Epoch: 271   Loss: 16.424158096313477\n",
      "Epoch: 272   Loss: 16.268314361572266\n",
      "Epoch: 273   Loss: 16.114356994628906\n",
      "Epoch: 274   Loss: 15.962270736694336\n",
      "Epoch: 275   Loss: 15.811994552612305\n",
      "Epoch: 276   Loss: 15.663528442382812\n",
      "Epoch: 277   Loss: 15.516899108886719\n",
      "Epoch: 278   Loss: 15.372007369995117\n",
      "Epoch: 279   Loss: 15.228853225708008\n",
      "Epoch: 280   Loss: 15.087434768676758\n",
      "Epoch: 281   Loss: 14.947721481323242\n",
      "Epoch: 282   Loss: 14.809657096862793\n",
      "Epoch: 283   Loss: 14.673291206359863\n",
      "Epoch: 284   Loss: 14.53856086730957\n",
      "Epoch: 285   Loss: 14.40545654296875\n",
      "Epoch: 286   Loss: 14.27392578125\n",
      "Epoch: 287   Loss: 14.143991470336914\n",
      "Epoch: 288   Loss: 14.015632629394531\n",
      "Epoch: 289   Loss: 13.888765335083008\n",
      "Epoch: 290   Loss: 13.763456344604492\n",
      "Epoch: 291   Loss: 13.63964557647705\n",
      "Epoch: 292   Loss: 13.517297744750977\n",
      "Epoch: 293   Loss: 13.396428108215332\n",
      "Epoch: 294   Loss: 13.277013778686523\n",
      "Epoch: 295   Loss: 13.159001350402832\n",
      "Epoch: 296   Loss: 13.042409896850586\n",
      "Epoch: 297   Loss: 12.927192687988281\n",
      "Epoch: 298   Loss: 12.813383102416992\n",
      "Epoch: 299   Loss: 12.700910568237305\n",
      "Epoch: 300   Loss: 12.58979606628418\n",
      "Epoch: 301   Loss: 12.479985237121582\n",
      "Epoch: 302   Loss: 12.37149429321289\n",
      "Epoch: 303   Loss: 12.264290809631348\n",
      "Epoch: 304   Loss: 12.158348083496094\n",
      "Epoch: 305   Loss: 12.053678512573242\n",
      "Epoch: 306   Loss: 11.950254440307617\n",
      "Epoch: 307   Loss: 11.848051071166992\n",
      "Epoch: 308   Loss: 11.747052192687988\n",
      "Epoch: 309   Loss: 11.647271156311035\n",
      "Epoch: 310   Loss: 11.548648834228516\n",
      "Epoch: 311   Loss: 11.451193809509277\n",
      "Epoch: 312   Loss: 11.354901313781738\n",
      "Epoch: 313   Loss: 11.25975227355957\n",
      "Epoch: 314   Loss: 11.165716171264648\n",
      "Epoch: 315   Loss: 11.072805404663086\n",
      "Epoch: 316   Loss: 10.980949401855469\n",
      "Epoch: 317   Loss: 10.890213012695312\n",
      "Epoch: 318   Loss: 10.800543785095215\n",
      "Epoch: 319   Loss: 10.71191120147705\n",
      "Epoch: 320   Loss: 10.624345779418945\n",
      "Epoch: 321   Loss: 10.537793159484863\n",
      "Epoch: 322   Loss: 10.452255249023438\n",
      "Epoch: 323   Loss: 10.367732048034668\n",
      "Epoch: 324   Loss: 10.284183502197266\n",
      "Epoch: 325   Loss: 10.201627731323242\n",
      "Epoch: 326   Loss: 10.120031356811523\n",
      "Epoch: 327   Loss: 10.03938102722168\n",
      "Epoch: 328   Loss: 9.959685325622559\n",
      "Epoch: 329   Loss: 9.880925178527832\n",
      "Epoch: 330   Loss: 9.803068161010742\n",
      "Epoch: 331   Loss: 9.726117134094238\n",
      "Epoch: 332   Loss: 9.650092124938965\n",
      "Epoch: 333   Loss: 9.574918746948242\n",
      "Epoch: 334   Loss: 9.50062084197998\n",
      "Epoch: 335   Loss: 9.427217483520508\n",
      "Epoch: 336   Loss: 9.35464096069336\n",
      "Epoch: 337   Loss: 9.282930374145508\n",
      "Epoch: 338   Loss: 9.212011337280273\n",
      "Epoch: 339   Loss: 9.141944885253906\n",
      "Epoch: 340   Loss: 9.072681427001953\n",
      "Epoch: 341   Loss: 9.004220962524414\n",
      "Epoch: 342   Loss: 8.936539649963379\n",
      "Epoch: 343   Loss: 8.869651794433594\n",
      "Epoch: 344   Loss: 8.803518295288086\n",
      "Epoch: 345   Loss: 8.738158226013184\n",
      "Epoch: 346   Loss: 8.673538208007812\n",
      "Epoch: 347   Loss: 8.609671592712402\n",
      "Epoch: 348   Loss: 8.546549797058105\n",
      "Epoch: 349   Loss: 8.484128952026367\n",
      "Epoch: 350   Loss: 8.422436714172363\n",
      "Epoch: 351   Loss: 8.361455917358398\n",
      "Epoch: 352   Loss: 8.301163673400879\n",
      "Epoch: 353   Loss: 8.241568565368652\n",
      "Epoch: 354   Loss: 8.182645797729492\n",
      "Epoch: 355   Loss: 8.1243896484375\n",
      "Epoch: 356   Loss: 8.066786766052246\n",
      "Epoch: 357   Loss: 8.009870529174805\n",
      "Epoch: 358   Loss: 7.953586578369141\n",
      "Epoch: 359   Loss: 7.897950172424316\n",
      "Epoch: 360   Loss: 7.84292459487915\n",
      "Epoch: 361   Loss: 7.788524627685547\n",
      "Epoch: 362   Loss: 7.734755516052246\n",
      "Epoch: 363   Loss: 7.681582450866699\n",
      "Epoch: 364   Loss: 7.629019260406494\n",
      "Epoch: 365   Loss: 7.577065944671631\n",
      "Epoch: 366   Loss: 7.525671482086182\n",
      "Epoch: 367   Loss: 7.474863529205322\n",
      "Epoch: 368   Loss: 7.424623012542725\n",
      "Epoch: 369   Loss: 7.374941349029541\n",
      "Epoch: 370   Loss: 7.325824737548828\n",
      "Epoch: 371   Loss: 7.277254581451416\n",
      "Epoch: 372   Loss: 7.229244232177734\n",
      "Epoch: 373   Loss: 7.18175745010376\n",
      "Epoch: 374   Loss: 7.134797096252441\n",
      "Epoch: 375   Loss: 7.088353633880615\n",
      "Epoch: 376   Loss: 7.042445182800293\n",
      "Epoch: 377   Loss: 6.997035980224609\n",
      "Epoch: 378   Loss: 6.952136993408203\n",
      "Epoch: 379   Loss: 6.907732963562012\n",
      "Epoch: 380   Loss: 6.863818168640137\n",
      "Epoch: 381   Loss: 6.820390224456787\n",
      "Epoch: 382   Loss: 6.77743673324585\n",
      "Epoch: 383   Loss: 6.7349677085876465\n",
      "Epoch: 384   Loss: 6.69296932220459\n",
      "Epoch: 385   Loss: 6.651419639587402\n",
      "Epoch: 386   Loss: 6.610352993011475\n",
      "Epoch: 387   Loss: 6.569708824157715\n",
      "Epoch: 388   Loss: 6.529515266418457\n",
      "Epoch: 389   Loss: 6.48977518081665\n",
      "Epoch: 390   Loss: 6.4504499435424805\n",
      "Epoch: 391   Loss: 6.411558628082275\n",
      "Epoch: 392   Loss: 6.3730926513671875\n",
      "Epoch: 393   Loss: 6.33505392074585\n",
      "Epoch: 394   Loss: 6.297434329986572\n",
      "Epoch: 395   Loss: 6.260194301605225\n",
      "Epoch: 396   Loss: 6.2233805656433105\n",
      "Epoch: 397   Loss: 6.186955451965332\n",
      "Epoch: 398   Loss: 6.150940895080566\n",
      "Epoch: 399   Loss: 6.11529016494751\n",
      "Epoch: 400   Loss: 6.080028057098389\n",
      "Epoch: 401   Loss: 6.04516077041626\n",
      "Epoch: 402   Loss: 6.010645866394043\n",
      "Epoch: 403   Loss: 5.976503849029541\n",
      "Epoch: 404   Loss: 5.942732810974121\n",
      "Epoch: 405   Loss: 5.909337043762207\n",
      "Epoch: 406   Loss: 5.876286506652832\n",
      "Epoch: 407   Loss: 5.843583583831787\n",
      "Epoch: 408   Loss: 5.811230182647705\n",
      "Epoch: 409   Loss: 5.77922248840332\n",
      "Epoch: 410   Loss: 5.74755859375\n",
      "Epoch: 411   Loss: 5.716226100921631\n",
      "Epoch: 412   Loss: 5.685205936431885\n",
      "Epoch: 413   Loss: 5.654534816741943\n",
      "Epoch: 414   Loss: 5.624171257019043\n",
      "Epoch: 415   Loss: 5.594141006469727\n",
      "Epoch: 416   Loss: 5.56441593170166\n",
      "Epoch: 417   Loss: 5.535001277923584\n",
      "Epoch: 418   Loss: 5.505911827087402\n",
      "Epoch: 419   Loss: 5.477118968963623\n",
      "Epoch: 420   Loss: 5.448609352111816\n",
      "Epoch: 421   Loss: 5.420402526855469\n",
      "Epoch: 422   Loss: 5.392500877380371\n",
      "Epoch: 423   Loss: 5.364871025085449\n",
      "Epoch: 424   Loss: 5.3375372886657715\n",
      "Epoch: 425   Loss: 5.3104753494262695\n",
      "Epoch: 426   Loss: 5.283706188201904\n",
      "Epoch: 427   Loss: 5.257214546203613\n",
      "Epoch: 428   Loss: 5.230980396270752\n",
      "Epoch: 429   Loss: 5.205010890960693\n",
      "Epoch: 430   Loss: 5.179319381713867\n",
      "Epoch: 431   Loss: 5.153875350952148\n",
      "Epoch: 432   Loss: 5.128711223602295\n",
      "Epoch: 433   Loss: 5.103794097900391\n",
      "Epoch: 434   Loss: 5.079129219055176\n",
      "Epoch: 435   Loss: 5.054698467254639\n",
      "Epoch: 436   Loss: 5.03053617477417\n",
      "Epoch: 437   Loss: 5.006600379943848\n",
      "Epoch: 438   Loss: 4.982901573181152\n",
      "Epoch: 439   Loss: 4.959460735321045\n",
      "Epoch: 440   Loss: 4.936244010925293\n",
      "Epoch: 441   Loss: 4.9132537841796875\n",
      "Epoch: 442   Loss: 4.890499591827393\n",
      "Epoch: 443   Loss: 4.867959976196289\n",
      "Epoch: 444   Loss: 4.845667839050293\n",
      "Epoch: 445   Loss: 4.823572635650635\n",
      "Epoch: 446   Loss: 4.801697254180908\n",
      "Epoch: 447   Loss: 4.780050754547119\n",
      "Epoch: 448   Loss: 4.75860595703125\n",
      "Epoch: 449   Loss: 4.737380027770996\n",
      "Epoch: 450   Loss: 4.716358184814453\n",
      "Epoch: 451   Loss: 4.695553302764893\n",
      "Epoch: 452   Loss: 4.674924850463867\n",
      "Epoch: 453   Loss: 4.654520511627197\n",
      "Epoch: 454   Loss: 4.6342902183532715\n",
      "Epoch: 455   Loss: 4.61427640914917\n",
      "Epoch: 456   Loss: 4.594454288482666\n",
      "Epoch: 457   Loss: 4.574815273284912\n",
      "Epoch: 458   Loss: 4.555352210998535\n",
      "Epoch: 459   Loss: 4.536086082458496\n",
      "Epoch: 460   Loss: 4.5170111656188965\n",
      "Epoch: 461   Loss: 4.498136520385742\n",
      "Epoch: 462   Loss: 4.479393005371094\n",
      "Epoch: 463   Loss: 4.460858345031738\n",
      "Epoch: 464   Loss: 4.442488670349121\n",
      "Epoch: 465   Loss: 4.424301624298096\n",
      "Epoch: 466   Loss: 4.406275749206543\n",
      "Epoch: 467   Loss: 4.388424873352051\n",
      "Epoch: 468   Loss: 4.3707380294799805\n",
      "Epoch: 469   Loss: 4.353216648101807\n",
      "Epoch: 470   Loss: 4.335859775543213\n",
      "Epoch: 471   Loss: 4.31868314743042\n",
      "Epoch: 472   Loss: 4.301626682281494\n",
      "Epoch: 473   Loss: 4.284743309020996\n",
      "Epoch: 474   Loss: 4.268033027648926\n",
      "Epoch: 475   Loss: 4.251456260681152\n",
      "Epoch: 476   Loss: 4.235043525695801\n",
      "Epoch: 477   Loss: 4.218778610229492\n",
      "Epoch: 478   Loss: 4.2026495933532715\n",
      "Epoch: 479   Loss: 4.186676979064941\n",
      "Epoch: 480   Loss: 4.170841693878174\n",
      "Epoch: 481   Loss: 4.155158996582031\n",
      "Epoch: 482   Loss: 4.139611721038818\n",
      "Epoch: 483   Loss: 4.124207973480225\n",
      "Epoch: 484   Loss: 4.1089348793029785\n",
      "Epoch: 485   Loss: 4.093793869018555\n",
      "Epoch: 486   Loss: 4.0788068771362305\n",
      "Epoch: 487   Loss: 4.06394624710083\n",
      "Epoch: 488   Loss: 4.049209117889404\n",
      "Epoch: 489   Loss: 4.034612655639648\n",
      "Epoch: 490   Loss: 4.020132541656494\n",
      "Epoch: 491   Loss: 4.00578498840332\n",
      "Epoch: 492   Loss: 3.9915623664855957\n",
      "Epoch: 493   Loss: 3.977454423904419\n",
      "Epoch: 494   Loss: 3.963475465774536\n",
      "Epoch: 495   Loss: 3.9496207237243652\n",
      "Epoch: 496   Loss: 3.935896396636963\n",
      "Epoch: 497   Loss: 3.9222731590270996\n",
      "Epoch: 498   Loss: 3.9087767601013184\n",
      "Epoch: 499   Loss: 3.8953871726989746\n",
      "Epoch: 500   Loss: 3.882108688354492\n",
      "Epoch: 501   Loss: 3.8689472675323486\n",
      "Epoch: 502   Loss: 3.8559012413024902\n",
      "Epoch: 503   Loss: 3.842968702316284\n",
      "Epoch: 504   Loss: 3.830136775970459\n",
      "Epoch: 505   Loss: 3.817409038543701\n",
      "Epoch: 506   Loss: 3.8048062324523926\n",
      "Epoch: 507   Loss: 3.7922873497009277\n",
      "Epoch: 508   Loss: 3.7798736095428467\n",
      "Epoch: 509   Loss: 3.7675774097442627\n",
      "Epoch: 510   Loss: 3.7553584575653076\n",
      "Epoch: 511   Loss: 3.7432656288146973\n",
      "Epoch: 512   Loss: 3.7312560081481934\n",
      "Epoch: 513   Loss: 3.719353199005127\n",
      "Epoch: 514   Loss: 3.7075507640838623\n",
      "Epoch: 515   Loss: 3.695830821990967\n",
      "Epoch: 516   Loss: 3.6842072010040283\n",
      "Epoch: 517   Loss: 3.6726748943328857\n",
      "Epoch: 518   Loss: 3.661240816116333\n",
      "Epoch: 519   Loss: 3.649897336959839\n",
      "Epoch: 520   Loss: 3.6386520862579346\n",
      "Epoch: 521   Loss: 3.6274898052215576\n",
      "Epoch: 522   Loss: 3.616398572921753\n",
      "Epoch: 523   Loss: 3.6054184436798096\n",
      "Epoch: 524   Loss: 3.594520092010498\n",
      "Epoch: 525   Loss: 3.583690643310547\n",
      "Epoch: 526   Loss: 3.5729568004608154\n",
      "Epoch: 527   Loss: 3.5623130798339844\n",
      "Epoch: 528   Loss: 3.5517401695251465\n",
      "Epoch: 529   Loss: 3.54126238822937\n",
      "Epoch: 530   Loss: 3.530862331390381\n",
      "Epoch: 531   Loss: 3.5205283164978027\n",
      "Epoch: 532   Loss: 3.5102882385253906\n",
      "Epoch: 533   Loss: 3.500108242034912\n",
      "Epoch: 534   Loss: 3.4900214672088623\n",
      "Epoch: 535   Loss: 3.4800045490264893\n",
      "Epoch: 536   Loss: 3.470048427581787\n",
      "Epoch: 537   Loss: 3.4601967334747314\n",
      "Epoch: 538   Loss: 3.450404405593872\n",
      "Epoch: 539   Loss: 3.44067645072937\n",
      "Epoch: 540   Loss: 3.431018114089966\n",
      "Epoch: 541   Loss: 3.4214415550231934\n",
      "Epoch: 542   Loss: 3.4119389057159424\n",
      "Epoch: 543   Loss: 3.402489185333252\n",
      "Epoch: 544   Loss: 3.3931164741516113\n",
      "Epoch: 545   Loss: 3.3838233947753906\n",
      "Epoch: 546   Loss: 3.3745803833007812\n",
      "Epoch: 547   Loss: 3.365403652191162\n",
      "Epoch: 548   Loss: 3.3562989234924316\n",
      "Epoch: 549   Loss: 3.3472437858581543\n",
      "Epoch: 550   Loss: 3.3382744789123535\n",
      "Epoch: 551   Loss: 3.329350233078003\n",
      "Epoch: 552   Loss: 3.320505142211914\n",
      "Epoch: 553   Loss: 3.3117222785949707\n",
      "Epoch: 554   Loss: 3.3029918670654297\n",
      "Epoch: 555   Loss: 3.29431414604187\n",
      "Epoch: 556   Loss: 3.285709857940674\n",
      "Epoch: 557   Loss: 3.277165651321411\n",
      "Epoch: 558   Loss: 3.268665313720703\n",
      "Epoch: 559   Loss: 3.260237216949463\n",
      "Epoch: 560   Loss: 3.251877546310425\n",
      "Epoch: 561   Loss: 3.243537425994873\n",
      "Epoch: 562   Loss: 3.2352936267852783\n",
      "Epoch: 563   Loss: 3.227092742919922\n",
      "Epoch: 564   Loss: 3.2189412117004395\n",
      "Epoch: 565   Loss: 3.2108378410339355\n",
      "Epoch: 566   Loss: 3.2027974128723145\n",
      "Epoch: 567   Loss: 3.1948044300079346\n",
      "Epoch: 568   Loss: 3.186868667602539\n",
      "Epoch: 569   Loss: 3.1789727210998535\n",
      "Epoch: 570   Loss: 3.1711478233337402\n",
      "Epoch: 571   Loss: 3.163353681564331\n",
      "Epoch: 572   Loss: 3.1556167602539062\n",
      "Epoch: 573   Loss: 3.147937774658203\n",
      "Epoch: 574   Loss: 3.140293836593628\n",
      "Epoch: 575   Loss: 3.1327202320098877\n",
      "Epoch: 576   Loss: 3.1251606941223145\n",
      "Epoch: 577   Loss: 3.1176705360412598\n",
      "Epoch: 578   Loss: 3.110236883163452\n",
      "Epoch: 579   Loss: 3.1028311252593994\n",
      "Epoch: 580   Loss: 3.095472812652588\n",
      "Epoch: 581   Loss: 3.088179349899292\n",
      "Epoch: 582   Loss: 3.0809121131896973\n",
      "Epoch: 583   Loss: 3.073704481124878\n",
      "Epoch: 584   Loss: 3.066516399383545\n",
      "Epoch: 585   Loss: 3.0593888759613037\n",
      "Epoch: 586   Loss: 3.0523009300231934\n",
      "Epoch: 587   Loss: 3.0452613830566406\n",
      "Epoch: 588   Loss: 3.038257122039795\n",
      "Epoch: 589   Loss: 3.0313048362731934\n",
      "Epoch: 590   Loss: 3.024383783340454\n",
      "Epoch: 591   Loss: 3.017507553100586\n",
      "Epoch: 592   Loss: 3.0106654167175293\n",
      "Epoch: 593   Loss: 3.0038695335388184\n",
      "Epoch: 594   Loss: 2.997114658355713\n",
      "Epoch: 595   Loss: 2.990400552749634\n",
      "Epoch: 596   Loss: 2.983720302581787\n",
      "Epoch: 597   Loss: 2.9770798683166504\n",
      "Epoch: 598   Loss: 2.9704718589782715\n",
      "Epoch: 599   Loss: 2.9639010429382324\n",
      "Epoch: 600   Loss: 2.957381010055542\n",
      "Epoch: 601   Loss: 2.950882911682129\n",
      "Epoch: 602   Loss: 2.9444327354431152\n",
      "Epoch: 603   Loss: 2.9380130767822266\n",
      "Epoch: 604   Loss: 2.9316344261169434\n",
      "Epoch: 605   Loss: 2.925278425216675\n",
      "Epoch: 606   Loss: 2.918964147567749\n",
      "Epoch: 607   Loss: 2.912686586380005\n",
      "Epoch: 608   Loss: 2.9064435958862305\n",
      "Epoch: 609   Loss: 2.900240421295166\n",
      "Epoch: 610   Loss: 2.8940632343292236\n",
      "Epoch: 611   Loss: 2.887916088104248\n",
      "Epoch: 612   Loss: 2.881805181503296\n",
      "Epoch: 613   Loss: 2.8757362365722656\n",
      "Epoch: 614   Loss: 2.869683265686035\n",
      "Epoch: 615   Loss: 2.863677501678467\n",
      "Epoch: 616   Loss: 2.857692003250122\n",
      "Epoch: 617   Loss: 2.851741313934326\n",
      "Epoch: 618   Loss: 2.8458290100097656\n",
      "Epoch: 619   Loss: 2.8399269580841064\n",
      "Epoch: 620   Loss: 2.8340795040130615\n",
      "Epoch: 621   Loss: 2.828242778778076\n",
      "Epoch: 622   Loss: 2.822444438934326\n",
      "Epoch: 623   Loss: 2.816664457321167\n",
      "Epoch: 624   Loss: 2.810938596725464\n",
      "Epoch: 625   Loss: 2.805222988128662\n",
      "Epoch: 626   Loss: 2.7995448112487793\n",
      "Epoch: 627   Loss: 2.7938809394836426\n",
      "Epoch: 628   Loss: 2.7882561683654785\n",
      "Epoch: 629   Loss: 2.7826597690582275\n",
      "Epoch: 630   Loss: 2.7771027088165283\n",
      "Epoch: 631   Loss: 2.7715561389923096\n",
      "Epoch: 632   Loss: 2.7660393714904785\n",
      "Epoch: 633   Loss: 2.7605414390563965\n",
      "Epoch: 634   Loss: 2.7550859451293945\n",
      "Epoch: 635   Loss: 2.7496368885040283\n",
      "Epoch: 636   Loss: 2.7442307472229004\n",
      "Epoch: 637   Loss: 2.7388505935668945\n",
      "Epoch: 638   Loss: 2.7334823608398438\n",
      "Epoch: 639   Loss: 2.7281479835510254\n",
      "Epoch: 640   Loss: 2.7228355407714844\n",
      "Epoch: 641   Loss: 2.7175586223602295\n",
      "Epoch: 642   Loss: 2.7122998237609863\n",
      "Epoch: 643   Loss: 2.707049608230591\n",
      "Epoch: 644   Loss: 2.7018446922302246\n",
      "Epoch: 645   Loss: 2.696657657623291\n",
      "Epoch: 646   Loss: 2.691493511199951\n",
      "Epoch: 647   Loss: 2.6863484382629395\n",
      "Epoch: 648   Loss: 2.681225299835205\n",
      "Epoch: 649   Loss: 2.6761324405670166\n",
      "Epoch: 650   Loss: 2.6710598468780518\n",
      "Epoch: 651   Loss: 2.666006088256836\n",
      "Epoch: 652   Loss: 2.660977840423584\n",
      "Epoch: 653   Loss: 2.655982494354248\n",
      "Epoch: 654   Loss: 2.6509928703308105\n",
      "Epoch: 655   Loss: 2.646043062210083\n",
      "Epoch: 656   Loss: 2.64109468460083\n",
      "Epoch: 657   Loss: 2.636178970336914\n",
      "Epoch: 658   Loss: 2.6312763690948486\n",
      "Epoch: 659   Loss: 2.626399278640747\n",
      "Epoch: 660   Loss: 2.6215415000915527\n",
      "Epoch: 661   Loss: 2.616706371307373\n",
      "Epoch: 662   Loss: 2.611888885498047\n",
      "Epoch: 663   Loss: 2.607100486755371\n",
      "Epoch: 664   Loss: 2.6023192405700684\n",
      "Epoch: 665   Loss: 2.5975499153137207\n",
      "Epoch: 666   Loss: 2.592825412750244\n",
      "Epoch: 667   Loss: 2.588111639022827\n",
      "Epoch: 668   Loss: 2.58341908454895\n",
      "Epoch: 669   Loss: 2.578726291656494\n",
      "Epoch: 670   Loss: 2.574066162109375\n",
      "Epoch: 671   Loss: 2.5694339275360107\n",
      "Epoch: 672   Loss: 2.564805269241333\n",
      "Epoch: 673   Loss: 2.5602073669433594\n",
      "Epoch: 674   Loss: 2.555621385574341\n",
      "Epoch: 675   Loss: 2.5510594844818115\n",
      "Epoch: 676   Loss: 2.5465168952941895\n",
      "Epoch: 677   Loss: 2.5419764518737793\n",
      "Epoch: 678   Loss: 2.5374698638916016\n",
      "Epoch: 679   Loss: 2.5329666137695312\n",
      "Epoch: 680   Loss: 2.5284931659698486\n",
      "Epoch: 681   Loss: 2.524022340774536\n",
      "Epoch: 682   Loss: 2.5195858478546143\n",
      "Epoch: 683   Loss: 2.515151262283325\n",
      "Epoch: 684   Loss: 2.5107479095458984\n",
      "Epoch: 685   Loss: 2.5063633918762207\n",
      "Epoch: 686   Loss: 2.5019843578338623\n",
      "Epoch: 687   Loss: 2.4976134300231934\n",
      "Epoch: 688   Loss: 2.4932782649993896\n",
      "Epoch: 689   Loss: 2.48895263671875\n",
      "Epoch: 690   Loss: 2.4846415519714355\n",
      "Epoch: 691   Loss: 2.4803550243377686\n",
      "Epoch: 692   Loss: 2.476062297821045\n",
      "Epoch: 693   Loss: 2.47180438041687\n",
      "Epoch: 694   Loss: 2.4675540924072266\n",
      "Epoch: 695   Loss: 2.463326930999756\n",
      "Epoch: 696   Loss: 2.459104061126709\n",
      "Epoch: 697   Loss: 2.4549036026000977\n",
      "Epoch: 698   Loss: 2.450711488723755\n",
      "Epoch: 699   Loss: 2.4465389251708984\n",
      "Epoch: 700   Loss: 2.442383050918579\n",
      "Epoch: 701   Loss: 2.4382309913635254\n",
      "Epoch: 702   Loss: 2.4341044425964355\n",
      "Epoch: 703   Loss: 2.4299912452697754\n",
      "Epoch: 704   Loss: 2.4258906841278076\n",
      "Epoch: 705   Loss: 2.4218103885650635\n",
      "Epoch: 706   Loss: 2.4177355766296387\n",
      "Epoch: 707   Loss: 2.413670778274536\n",
      "Epoch: 708   Loss: 2.4096360206604004\n",
      "Epoch: 709   Loss: 2.4056074619293213\n",
      "Epoch: 710   Loss: 2.4015846252441406\n",
      "Epoch: 711   Loss: 2.3975913524627686\n",
      "Epoch: 712   Loss: 2.393609046936035\n",
      "Epoch: 713   Loss: 2.389622926712036\n",
      "Epoch: 714   Loss: 2.385659694671631\n",
      "Epoch: 715   Loss: 2.381711483001709\n",
      "Epoch: 716   Loss: 2.3777713775634766\n",
      "Epoch: 717   Loss: 2.373863935470581\n",
      "Epoch: 718   Loss: 2.369941234588623\n",
      "Epoch: 719   Loss: 2.3660471439361572\n",
      "Epoch: 720   Loss: 2.3621580600738525\n",
      "Epoch: 721   Loss: 2.3582863807678223\n",
      "Epoch: 722   Loss: 2.354428291320801\n",
      "Epoch: 723   Loss: 2.35056734085083\n",
      "Epoch: 724   Loss: 2.346742868423462\n",
      "Epoch: 725   Loss: 2.342923402786255\n",
      "Epoch: 726   Loss: 2.3391122817993164\n",
      "Epoch: 727   Loss: 2.3353118896484375\n",
      "Epoch: 728   Loss: 2.3315229415893555\n",
      "Epoch: 729   Loss: 2.3277578353881836\n",
      "Epoch: 730   Loss: 2.323991298675537\n",
      "Epoch: 731   Loss: 2.3202435970306396\n",
      "Epoch: 732   Loss: 2.3165040016174316\n",
      "Epoch: 733   Loss: 2.312777042388916\n",
      "Epoch: 734   Loss: 2.3090641498565674\n",
      "Epoch: 735   Loss: 2.305358648300171\n",
      "Epoch: 736   Loss: 2.30165433883667\n",
      "Epoch: 737   Loss: 2.2979767322540283\n",
      "Epoch: 738   Loss: 2.2943015098571777\n",
      "Epoch: 739   Loss: 2.290642738342285\n",
      "Epoch: 740   Loss: 2.2869906425476074\n",
      "Epoch: 741   Loss: 2.2833542823791504\n",
      "Epoch: 742   Loss: 2.2797272205352783\n",
      "Epoch: 743   Loss: 2.2761166095733643\n",
      "Epoch: 744   Loss: 2.2725024223327637\n",
      "Epoch: 745   Loss: 2.2689108848571777\n",
      "Epoch: 746   Loss: 2.2653298377990723\n",
      "Epoch: 747   Loss: 2.2617623805999756\n",
      "Epoch: 748   Loss: 2.258195638656616\n",
      "Epoch: 749   Loss: 2.254645347595215\n",
      "Epoch: 750   Loss: 2.251101016998291\n",
      "Epoch: 751   Loss: 2.2475576400756836\n",
      "Epoch: 752   Loss: 2.2440452575683594\n",
      "Epoch: 753   Loss: 2.2405343055725098\n",
      "Epoch: 754   Loss: 2.2370376586914062\n",
      "Epoch: 755   Loss: 2.233546257019043\n",
      "Epoch: 756   Loss: 2.2300608158111572\n",
      "Epoch: 757   Loss: 2.2265825271606445\n",
      "Epoch: 758   Loss: 2.223114490509033\n",
      "Epoch: 759   Loss: 2.219661235809326\n",
      "Epoch: 760   Loss: 2.2162280082702637\n",
      "Epoch: 761   Loss: 2.2127881050109863\n",
      "Epoch: 762   Loss: 2.2093722820281982\n",
      "Epoch: 763   Loss: 2.205958843231201\n",
      "Epoch: 764   Loss: 2.202549457550049\n",
      "Epoch: 765   Loss: 2.199150800704956\n",
      "Epoch: 766   Loss: 2.195786714553833\n",
      "Epoch: 767   Loss: 2.1924071311950684\n",
      "Epoch: 768   Loss: 2.189044713973999\n",
      "Epoch: 769   Loss: 2.185680866241455\n",
      "Epoch: 770   Loss: 2.1823313236236572\n",
      "Epoch: 771   Loss: 2.1789989471435547\n",
      "Epoch: 772   Loss: 2.1756703853607178\n",
      "Epoch: 773   Loss: 2.172339677810669\n",
      "Epoch: 774   Loss: 2.1690235137939453\n",
      "Epoch: 775   Loss: 2.165717601776123\n",
      "Epoch: 776   Loss: 2.1624295711517334\n",
      "Epoch: 777   Loss: 2.1591403484344482\n",
      "Epoch: 778   Loss: 2.1558761596679688\n",
      "Epoch: 779   Loss: 2.1525938510894775\n",
      "Epoch: 780   Loss: 2.1493356227874756\n",
      "Epoch: 781   Loss: 2.1460838317871094\n",
      "Epoch: 782   Loss: 2.142847776412964\n",
      "Epoch: 783   Loss: 2.1396021842956543\n",
      "Epoch: 784   Loss: 2.1363816261291504\n",
      "Epoch: 785   Loss: 2.1331627368927\n",
      "Epoch: 786   Loss: 2.1299545764923096\n",
      "Epoch: 787   Loss: 2.126753330230713\n",
      "Epoch: 788   Loss: 2.1235644817352295\n",
      "Epoch: 789   Loss: 2.120384693145752\n",
      "Epoch: 790   Loss: 2.1172053813934326\n",
      "Epoch: 791   Loss: 2.114037036895752\n",
      "Epoch: 792   Loss: 2.110879421234131\n",
      "Epoch: 793   Loss: 2.1077191829681396\n",
      "Epoch: 794   Loss: 2.1045708656311035\n",
      "Epoch: 795   Loss: 2.1014347076416016\n",
      "Epoch: 796   Loss: 2.0983142852783203\n",
      "Epoch: 797   Loss: 2.095188617706299\n",
      "Epoch: 798   Loss: 2.0920863151550293\n",
      "Epoch: 799   Loss: 2.0889720916748047\n",
      "Epoch: 800   Loss: 2.0858829021453857\n",
      "Epoch: 801   Loss: 2.0827889442443848\n",
      "Epoch: 802   Loss: 2.079718828201294\n",
      "Epoch: 803   Loss: 2.0766520500183105\n",
      "Epoch: 804   Loss: 2.073575496673584\n",
      "Epoch: 805   Loss: 2.070526361465454\n",
      "Epoch: 806   Loss: 2.0674643516540527\n",
      "Epoch: 807   Loss: 2.0644278526306152\n",
      "Epoch: 808   Loss: 2.061387538909912\n",
      "Epoch: 809   Loss: 2.058363676071167\n",
      "Epoch: 810   Loss: 2.0553486347198486\n",
      "Epoch: 811   Loss: 2.052330732345581\n",
      "Epoch: 812   Loss: 2.0493216514587402\n",
      "Epoch: 813   Loss: 2.046309232711792\n",
      "Epoch: 814   Loss: 2.043330430984497\n",
      "Epoch: 815   Loss: 2.040341377258301\n",
      "Epoch: 816   Loss: 2.0373663902282715\n",
      "Epoch: 817   Loss: 2.0343990325927734\n",
      "Epoch: 818   Loss: 2.0314297676086426\n",
      "Epoch: 819   Loss: 2.0284769535064697\n",
      "Epoch: 820   Loss: 2.025527000427246\n",
      "Epoch: 821   Loss: 2.022578716278076\n",
      "Epoch: 822   Loss: 2.0196473598480225\n",
      "Epoch: 823   Loss: 2.016721725463867\n",
      "Epoch: 824   Loss: 2.0138015747070312\n",
      "Epoch: 825   Loss: 2.0108869075775146\n",
      "Epoch: 826   Loss: 2.0079846382141113\n",
      "Epoch: 827   Loss: 2.005074977874756\n",
      "Epoch: 828   Loss: 2.002185106277466\n",
      "Epoch: 829   Loss: 1.9992977380752563\n",
      "Epoch: 830   Loss: 1.9964072704315186\n",
      "Epoch: 831   Loss: 1.9935400485992432\n",
      "Epoch: 832   Loss: 1.9906717538833618\n",
      "Epoch: 833   Loss: 1.9878097772598267\n",
      "Epoch: 834   Loss: 1.9849555492401123\n",
      "Epoch: 835   Loss: 1.9821077585220337\n",
      "Epoch: 836   Loss: 1.9792697429656982\n",
      "Epoch: 837   Loss: 1.9764289855957031\n",
      "Epoch: 838   Loss: 1.9736047983169556\n",
      "Epoch: 839   Loss: 1.9707845449447632\n",
      "Epoch: 840   Loss: 1.9679733514785767\n",
      "Epoch: 841   Loss: 1.9651718139648438\n",
      "Epoch: 842   Loss: 1.9623708724975586\n",
      "Epoch: 843   Loss: 1.9595768451690674\n",
      "Epoch: 844   Loss: 1.9567883014678955\n",
      "Epoch: 845   Loss: 1.954001784324646\n",
      "Epoch: 846   Loss: 1.9512195587158203\n",
      "Epoch: 847   Loss: 1.9484630823135376\n",
      "Epoch: 848   Loss: 1.9456876516342163\n",
      "Epoch: 849   Loss: 1.9429343938827515\n",
      "Epoch: 850   Loss: 1.9401788711547852\n",
      "Epoch: 851   Loss: 1.9374268054962158\n",
      "Epoch: 852   Loss: 1.9346870183944702\n",
      "Epoch: 853   Loss: 1.9319597482681274\n",
      "Epoch: 854   Loss: 1.929222822189331\n",
      "Epoch: 855   Loss: 1.9265048503875732\n",
      "Epoch: 856   Loss: 1.923802137374878\n",
      "Epoch: 857   Loss: 1.9210903644561768\n",
      "Epoch: 858   Loss: 1.9183889627456665\n",
      "Epoch: 859   Loss: 1.915683388710022\n",
      "Epoch: 860   Loss: 1.9129949808120728\n",
      "Epoch: 861   Loss: 1.9103031158447266\n",
      "Epoch: 862   Loss: 1.9076364040374756\n",
      "Epoch: 863   Loss: 1.9049608707427979\n",
      "Epoch: 864   Loss: 1.9022881984710693\n",
      "Epoch: 865   Loss: 1.8996315002441406\n",
      "Epoch: 866   Loss: 1.8969758749008179\n",
      "Epoch: 867   Loss: 1.8943220376968384\n",
      "Epoch: 868   Loss: 1.8916804790496826\n",
      "Epoch: 869   Loss: 1.8890419006347656\n",
      "Epoch: 870   Loss: 1.886407494544983\n",
      "Epoch: 871   Loss: 1.8837776184082031\n",
      "Epoch: 872   Loss: 1.8811582326889038\n",
      "Epoch: 873   Loss: 1.8785377740859985\n",
      "Epoch: 874   Loss: 1.8759262561798096\n",
      "Epoch: 875   Loss: 1.8733268976211548\n",
      "Epoch: 876   Loss: 1.8707268238067627\n",
      "Epoch: 877   Loss: 1.8681352138519287\n",
      "Epoch: 878   Loss: 1.8655452728271484\n",
      "Epoch: 879   Loss: 1.862969994544983\n",
      "Epoch: 880   Loss: 1.8603922128677368\n",
      "Epoch: 881   Loss: 1.8578250408172607\n",
      "Epoch: 882   Loss: 1.8552541732788086\n",
      "Epoch: 883   Loss: 1.852696180343628\n",
      "Epoch: 884   Loss: 1.850145697593689\n",
      "Epoch: 885   Loss: 1.8475898504257202\n",
      "Epoch: 886   Loss: 1.845047950744629\n",
      "Epoch: 887   Loss: 1.8425180912017822\n",
      "Epoch: 888   Loss: 1.8399813175201416\n",
      "Epoch: 889   Loss: 1.8374465703964233\n",
      "Epoch: 890   Loss: 1.8349225521087646\n",
      "Epoch: 891   Loss: 1.832401990890503\n",
      "Epoch: 892   Loss: 1.8298940658569336\n",
      "Epoch: 893   Loss: 1.8273847103118896\n",
      "Epoch: 894   Loss: 1.8248817920684814\n",
      "Epoch: 895   Loss: 1.8223930597305298\n",
      "Epoch: 896   Loss: 1.819898009300232\n",
      "Epoch: 897   Loss: 1.8174183368682861\n",
      "Epoch: 898   Loss: 1.8149394989013672\n",
      "Epoch: 899   Loss: 1.8124637603759766\n",
      "Epoch: 900   Loss: 1.8099889755249023\n",
      "Epoch: 901   Loss: 1.807518720626831\n",
      "Epoch: 902   Loss: 1.8050639629364014\n",
      "Epoch: 903   Loss: 1.802610993385315\n",
      "Epoch: 904   Loss: 1.8001582622528076\n",
      "Epoch: 905   Loss: 1.797716498374939\n",
      "Epoch: 906   Loss: 1.7952795028686523\n",
      "Epoch: 907   Loss: 1.7928438186645508\n",
      "Epoch: 908   Loss: 1.790418267250061\n",
      "Epoch: 909   Loss: 1.7879890203475952\n",
      "Epoch: 910   Loss: 1.785570740699768\n",
      "Epoch: 911   Loss: 1.7831541299819946\n",
      "Epoch: 912   Loss: 1.7807514667510986\n",
      "Epoch: 913   Loss: 1.7783396244049072\n",
      "Epoch: 914   Loss: 1.7759422063827515\n",
      "Epoch: 915   Loss: 1.7735522985458374\n",
      "Epoch: 916   Loss: 1.771161437034607\n",
      "Epoch: 917   Loss: 1.7687747478485107\n",
      "Epoch: 918   Loss: 1.7663958072662354\n",
      "Epoch: 919   Loss: 1.764029860496521\n",
      "Epoch: 920   Loss: 1.7616596221923828\n",
      "Epoch: 921   Loss: 1.7592904567718506\n",
      "Epoch: 922   Loss: 1.756929636001587\n",
      "Epoch: 923   Loss: 1.7545735836029053\n",
      "Epoch: 924   Loss: 1.752223014831543\n",
      "Epoch: 925   Loss: 1.7498801946640015\n",
      "Epoch: 926   Loss: 1.7475366592407227\n",
      "Epoch: 927   Loss: 1.7451989650726318\n",
      "Epoch: 928   Loss: 1.7428638935089111\n",
      "Epoch: 929   Loss: 1.7405376434326172\n",
      "Epoch: 930   Loss: 1.738216757774353\n",
      "Epoch: 931   Loss: 1.735901117324829\n",
      "Epoch: 932   Loss: 1.7335853576660156\n",
      "Epoch: 933   Loss: 1.7312796115875244\n",
      "Epoch: 934   Loss: 1.7289711236953735\n",
      "Epoch: 935   Loss: 1.726676344871521\n",
      "Epoch: 936   Loss: 1.7243791818618774\n",
      "Epoch: 937   Loss: 1.7220890522003174\n",
      "Epoch: 938   Loss: 1.7198035717010498\n",
      "Epoch: 939   Loss: 1.7175260782241821\n",
      "Epoch: 940   Loss: 1.7152469158172607\n",
      "Epoch: 941   Loss: 1.7129837274551392\n",
      "Epoch: 942   Loss: 1.7107257843017578\n",
      "Epoch: 943   Loss: 1.7084524631500244\n",
      "Epoch: 944   Loss: 1.7061941623687744\n",
      "Epoch: 945   Loss: 1.703948974609375\n",
      "Epoch: 946   Loss: 1.7016944885253906\n",
      "Epoch: 947   Loss: 1.6994476318359375\n",
      "Epoch: 948   Loss: 1.697222352027893\n",
      "Epoch: 949   Loss: 1.6949779987335205\n",
      "Epoch: 950   Loss: 1.6927458047866821\n",
      "Epoch: 951   Loss: 1.6905181407928467\n",
      "Epoch: 952   Loss: 1.6882877349853516\n",
      "Epoch: 953   Loss: 1.6860793828964233\n",
      "Epoch: 954   Loss: 1.6838699579238892\n",
      "Epoch: 955   Loss: 1.6816555261611938\n",
      "Epoch: 956   Loss: 1.6794532537460327\n",
      "Epoch: 957   Loss: 1.677254319190979\n",
      "Epoch: 958   Loss: 1.6750640869140625\n",
      "Epoch: 959   Loss: 1.672865867614746\n",
      "Epoch: 960   Loss: 1.6706857681274414\n",
      "Epoch: 961   Loss: 1.6685116291046143\n",
      "Epoch: 962   Loss: 1.6663271188735962\n",
      "Epoch: 963   Loss: 1.6641525030136108\n",
      "Epoch: 964   Loss: 1.6619899272918701\n",
      "Epoch: 965   Loss: 1.6598246097564697\n",
      "Epoch: 966   Loss: 1.657662034034729\n",
      "Epoch: 967   Loss: 1.6555017232894897\n",
      "Epoch: 968   Loss: 1.6533454656600952\n",
      "Epoch: 969   Loss: 1.6512024402618408\n",
      "Epoch: 970   Loss: 1.6490589380264282\n",
      "Epoch: 971   Loss: 1.6469192504882812\n",
      "Epoch: 972   Loss: 1.6447839736938477\n",
      "Epoch: 973   Loss: 1.6426498889923096\n",
      "Epoch: 974   Loss: 1.6405346393585205\n",
      "Epoch: 975   Loss: 1.6384029388427734\n",
      "Epoch: 976   Loss: 1.6362807750701904\n",
      "Epoch: 977   Loss: 1.6341708898544312\n",
      "Epoch: 978   Loss: 1.6320607662200928\n",
      "Epoch: 979   Loss: 1.6299539804458618\n",
      "Epoch: 980   Loss: 1.6278444528579712\n",
      "Epoch: 981   Loss: 1.625762939453125\n",
      "Epoch: 982   Loss: 1.6236588954925537\n",
      "Epoch: 983   Loss: 1.6215693950653076\n",
      "Epoch: 984   Loss: 1.6194912195205688\n",
      "Epoch: 985   Loss: 1.61741042137146\n",
      "Epoch: 986   Loss: 1.6153331995010376\n",
      "Epoch: 987   Loss: 1.6132564544677734\n",
      "Epoch: 988   Loss: 1.6111913919448853\n",
      "Epoch: 989   Loss: 1.6091220378875732\n",
      "Epoch: 990   Loss: 1.6070607900619507\n",
      "Epoch: 991   Loss: 1.6050074100494385\n",
      "Epoch: 992   Loss: 1.6029443740844727\n",
      "Epoch: 993   Loss: 1.6008968353271484\n",
      "Epoch: 994   Loss: 1.5988553762435913\n",
      "Epoch: 995   Loss: 1.5968124866485596\n",
      "Epoch: 996   Loss: 1.5947811603546143\n",
      "Epoch: 997   Loss: 1.5927400588989258\n",
      "Epoch: 998   Loss: 1.590719223022461\n",
      "Epoch: 999   Loss: 1.5886865854263306\n"
     ]
    }
   ],
   "source": [
    "fit(1000,model,loss_fn,opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 57.2756,  70.3683],\n",
       "        [ 81.6816,  99.8089],\n",
       "        [119.7109, 134.8211],\n",
       "        [ 21.4578,  37.5748],\n",
       "        [100.8286, 117.2509]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
